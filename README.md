# é«˜ QPS ç¶²è·¯çˆ¬èŸ²ç³»çµ±

é€™æ˜¯ä¸€å€‹é«˜æ•ˆèƒ½çš„åˆ†æ•£å¼ç¶²è·¯çˆ¬èŸ²ç³»çµ±ï¼Œè¨­è¨ˆç›®æ¨™ç‚ºå¯¦ç¾ **æ¯ç§’ 10,000 æ¬¡æŸ¥è©¢ (QPS)**ï¼Œé©ç”¨æ–¼å¤§è¦æ¨¡ç¶²é æŠ“å–ã€‚æœ¬å€‰åº«åƒ…åŒ…å«æ ¸å¿ƒç¨‹å¼ç¢¼ï¼Œæ¡ç”¨ **Laravel**ã€**FastAPI**ã€**gRPC**ã€**Redis**ã€**Celery** å’Œ **PostgreSQL/MongoDB** æ§‹å»ºï¼Œå…·å‚™æ¨¡çµ„åŒ–ã€å¯æ“´å±•å’Œç”Ÿç”¢å°±ç·’ç‰¹æ€§ï¼Œæ”¯æ´åçˆ¬èŸ²å’Œç›£æ§åŠŸèƒ½ã€‚

---

## ğŸš€ å°ˆæ¡ˆæ¦‚è¿°

æœ¬å°ˆæ¡ˆæä¾›ä¸€å€‹åˆ†æ•£å¼çˆ¬èŸ²ç³»çµ±çš„æ ¸å¿ƒç¨‹å¼ç¢¼ï¼Œæ”¯æ´é«˜ QPS ä¸¦ç¢ºä¿ç©©å®šæ€§ã€‚ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š

- **é«˜ååé‡**ï¼šé€šé Docker Compose æ°´å¹³æ“´å±•ï¼Œæ”¯æ´ 10,000 QPSã€‚
- **æ¨¡çµ„åŒ–æ¶æ§‹**ï¼šåˆ†é›¢ API ç®¡ç† (Laravel)ã€çˆ¬èŸ²æ ¸å¿ƒ (FastAPI)ã€ä»»å‹™ä½‡åˆ— (Redis) å’Œè³‡æ–™å„²å­˜ (PostgreSQL/MongoDB)ã€‚
- **åçˆ¬èŸ²æªæ–½**ï¼šæ”¯æ´ä»£ç†æ± ã€éš¨æ©Ÿ User-Agent å’Œ 2Captcha è§£æ±º CAPTCHAã€‚
- **ç›£æ§**ï¼šæ•´åˆ Prometheusã€Grafana å’Œ Celery Flowerã€‚

**æ³¨æ„**ï¼šæœ¬å€‰åº«åƒ…åŒ…å«é—œéµç¨‹å¼ç¢¼ï¼ŒLaravel åŸºç¤ç’°å¢ƒéœ€è‡ªè¡Œå®‰è£ã€‚

---

## ğŸ› ï¸ ç³»çµ±æ¶æ§‹

ç³»çµ±åŒ…å«ä»¥ä¸‹æ ¸å¿ƒçµ„ä»¶ï¼š

1. **Laravel API** (`laravel_api/`)ï¼š
   - è™•ç†ä»»å‹™æäº¤å’Œç‹€æ…‹æŸ¥è©¢ï¼Œé€šé gRPC èˆ‡ FastAPI é€šä¿¡ã€‚
   - ä½¿ç”¨ Redis å„²å­˜ä»»å‹™ç‹€æ…‹ï¼ŒPostgreSQL/MongoDB å„²å­˜æ­·å²æ•¸æ“šã€‚

2. **FastAPI çˆ¬èŸ²æ ¸å¿ƒ** (`fastapi_crawler/`)ï¼š
   - åŸ·è¡ŒéåŒæ­¥ HTTP è«‹æ±‚ï¼Œç®¡ç†ä»£ç†æ± å’Œåçˆ¬èŸ²æªæ–½ã€‚
   - æä¾› gRPC ä¼ºæœå™¨å’Œ REST ç«¯é»ã€‚

3. **Celery å·¥ä½œç¯€é»** (`celery_workers/`)ï¼š
   - è™•ç†åˆ†æ•£å¼è³‡æ–™å„²å­˜ä»»å‹™ï¼Œæ”¯æ´é«˜ä¸¦ç™¼ã€‚

4. **Redis**ï¼šç®¡ç†ä»»å‹™ä½‡åˆ—ã€ä»£ç†æ± å’Œ URL å»é‡ã€‚
5. **PostgreSQL/MongoDB**ï¼šé è¨­ PostgreSQLï¼Œæ”¯æ´ MongoDBã€‚
6. **ç›£æ§**ï¼šPrometheusã€Grafana å’Œ Celery Flowerã€‚

---

## ğŸ“‹ å‰ç½®éœ€æ±‚

- **Docker** å’Œ **Docker Compose**ï¼šç”¨æ–¼å®¹å™¨åŒ–éƒ¨ç½²ã€‚
- **Laravel ç’°å¢ƒ**ï¼šPHP 8.2+ã€Composerï¼ˆéœ€è‡ªè¡Œå®‰è£ï¼‰ã€‚
- **ç¡¬é«”**ï¼ˆå»ºè­°ç”¨æ–¼ 10,000 QPSï¼‰ï¼š
  - CPUï¼š32-64 æ ¸å¿ƒã€‚
  - è¨˜æ†¶é«”ï¼š128-256 GBã€‚
  - ç¶²è·¯ï¼š10 Gbpsã€‚
  - å„²å­˜ï¼šNVMe SSDï¼ˆ1-2 TBï¼‰ã€‚
- **ä»£ç† API**ï¼šå¯é çš„ä»£ç†æä¾›å•†ï¼ˆå¦‚ 98IPã€BrightDataï¼‰ã€‚
- **2Captcha API é‡‘é‘°**ï¼ˆå¯é¸ï¼‰ï¼šç”¨æ–¼ CAPTCHA è§£æ±ºã€‚

---

## ğŸ—ï¸ éƒ¨ç½²æ­¥é©Ÿ

1. **å…‹éš†å€‰åº«**ï¼š
   ```bash
   git clone https://github.com/BpsEason/high_qps_crawler.git
   cd high_qps_crawler
   ```

2. **å®‰è£ Laravel åŸºç¤ç’°å¢ƒ**ï¼š
   - åœ¨ `laravel_api/` ç›®éŒ„ä¸‹ï¼Œå®‰è£ Laravel ä¾è³´ï¼š
     ```bash
     cd laravel_api
     composer install
     cp .env.example .env
     php artisan key:generate
     ```
   - é…ç½® `.env` ä¸­çš„è³‡æ–™åº«å’Œ Redis é€£ç·šï¼ˆè¦‹ä¸‹æ–‡ï¼‰ã€‚

3. **é…ç½®ç’°å¢ƒè®Šæ•¸**ï¼š
   - åœ¨ `fastapi_crawler/.env` ä¸­æ·»åŠ ï¼š
     ```env
     CAPTCHA_API_KEY=ä½ çš„_2captcha_api_é‡‘é‘°  # å¯é¸
     PROXY_API_URL=https://api.your-proxy-provider.com/get-proxies  # çœŸå¯¦ä»£ç† API
     PROXY_UPDATE_INTERVAL_SECONDS=3600  # ä»£ç†æ± æ›´æ–°é–“éš”ï¼ˆç§’ï¼‰
     ```
   - åœ¨ `laravel_api/.env` ä¸­é…ç½®ï¼š
     ```env
     DB_CONNECTION=pgsql
     DB_HOST=pgbouncer
     DB_PORT=6432
     DB_DATABASE=crawler_db
     DB_USERNAME=user
     DB_PASSWORD=password
     REDIS_HOST=redis
     REDIS_PORT=6379
     FASTAPI_GRPC_HOST=fastapi_crawler:50051
     ```

4. **é¸æ“‡è³‡æ–™åº«**ï¼š
   - é è¨­ï¼š**PostgreSQL**ï¼ˆæ­é… PgBouncerï¼‰ã€‚
   - è‹¥ä½¿ç”¨ **MongoDB**ï¼Œåœ¨ `docker-compose.yml` ä¸­ï¼š
     - è¨»é‡‹æ‰ `postgresql` å’Œ `pgbouncer`ã€‚
     - å–æ¶ˆ `mongodb` æœå‹™è¨»é‡‹ã€‚
     - æ›´æ–° `laravel_api/.env`ï¼š
       ```env
       DB_CONNECTION=mongodb
       DB_HOST=mongodb
       DB_PORT=27017
       DB_DATABASE=crawler_db
       MONGODB_URL=mongodb://mongodb:27017
       ```

5. **æ§‹å»ºä¸¦é‹è¡Œæœå‹™**ï¼š
   ```bash
   docker compose build
   docker compose up -d
   ```

6. **åˆå§‹åŒ–è³‡æ–™åº«**ï¼š
   ```bash
   docker compose exec laravel php artisan migrate
   ```

7. **æ“´å±• Celery å·¥ä½œç¯€é»**ï¼š
   ```bash
   docker compose up -d --scale celery_worker=10
   ```

8. **æ¸¬è©¦çˆ¬èŸ²**ï¼š
   - æäº¤ä»»å‹™ï¼š
     ```bash
     curl -X POST -H "Content-Type: application/json" -d '{"url": "http://example.com"}' http://localhost:8000/api/submit-crawl
     ```
   - æŸ¥è©¢ç‹€æ…‹ï¼š
     ```bash
     curl http://localhost:8000/api/crawl-status/{task_id}
     ```

---

## ğŸ”‘ é—œéµç¨‹å¼ç¢¼èˆ‡ä¸­æ–‡è¨»è§£

ä»¥ä¸‹æ˜¯æ ¸å¿ƒç¨‹å¼ç¢¼ç‰‡æ®µï¼ŒåŒ…å«ä¸­æ–‡è¨»è§£ï¼š

### 1. Laravel API - ä»»å‹™æäº¤ (`laravel_api/app/Http/Controllers/CrawlerController.php`)

```php
<?php
namespace App\Http\Controllers;

use Illuminate\Http\Request;
use Grpc\ChannelCredentials;
use App\Grpc\CrawlerServiceClient;
use Illuminate\Support\Facades\Log;
use Illuminate\Support\Facades\Redis;

class CrawlerController extends Controller
{
    /**
     * æäº¤çˆ¬å–ä»»å‹™è‡³ FastAPIï¼ˆé€šé gRPCï¼‰
     */
    public function submitCrawlTask(Request $request)
    {
        // é©—è­‰è¼¸å…¥ URL
        $request->validate(['url' => 'required|url']);
        $url = $request->input('url');
        $taskId = uniqid('crawl_'); // ç”Ÿæˆå”¯ä¸€ä»»å‹™ ID

        try {
            // åˆå§‹åŒ– gRPC å®¢æˆ¶ç«¯
            $client = new CrawlerServiceClient(env('FASTAPI_GRPC_HOST', 'fastapi_crawler:50051'), [
                'credentials' => ChannelCredentials::createInsecure(),
            ]);
            
            // å‰µå»º gRPC è«‹æ±‚
            $requestProto = new \Crawler\CrawlRequest();
            $requestProto->setUrl($url);
            $requestProto->setTaskId($taskId);

            // ç™¼é€ gRPC è«‹æ±‚
            list($response, $status) = $client->Crawl($requestProto)->wait();
            if ($status->code !== \Grpc\STATUS_OK) {
                throw new \Exception("gRPC èª¿ç”¨å¤±æ•—: " . $status->details);
            }

            // å„²å­˜ä»»å‹™ç‹€æ…‹è‡³ Redis
            Redis::hset("crawl_task_status:{$taskId}", "status", "dispatched");
            Redis::hset("crawl_task_status:{$taskId}", "url", $url);
            Redis::hset("crawl_task_status:{$taskId}", "dispatched_at", now()->toIso8601String());

            Log::info("gRPC æˆåŠŸï¼ŒURL: $url, ä»»å‹™ ID: $taskId");
            return response()->json([
                'message' => 'ä»»å‹™å·²æäº¤',
                'task_id' => $taskId,
                'grpc_response_status' => $response->getStatus(),
            ]);
        } catch (\Exception $e) {
            // è¨˜éŒ„éŒ¯èª¤ä¸¦æ›´æ–°ç‹€æ…‹
            Log::error("æäº¤å¤±æ•—ï¼ŒURL: $url. éŒ¯èª¤: " . $e->getMessage());
            Redis::hset("crawl_task_status:{$taskId}", "status", "submission_failed");
            Redis::hset("crawl_task_status:{$taskId}", "error", $e->getMessage());
            return response()->json(['error' => 'æäº¤å¤±æ•—: ' . $e->getMessage()], 500);
        }
    }
}
```

**èªªæ˜**ï¼šè™•ç†ä»»å‹™æäº¤ï¼Œé€šé gRPC èˆ‡ FastAPI é€šä¿¡ï¼Œä¸¦ä½¿ç”¨ Redis å„²å­˜ä»»å‹™ç‹€æ…‹ã€‚

---

### 2. FastAPI çˆ¬èŸ²æ ¸å¿ƒ - ç¶²é æŠ“å– (`fastapi_crawler/app/crawler.py`)

```python
import aiohttp
import httpx
from fake_useragent import UserAgent
import asyncio
import redis.asyncio as aioredis
import random
import os

REDIS_HOST = os.getenv("REDIS_HOST", "redis")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))

async def get_redis_client():
    """ç²å– Redis é€£ç·š"""
    global _redis_client
    if _redis_client is None:
        _redis_client = await aioredis.from_url(f"redis://{REDIS_HOST}:{REDIS_PORT}")
    return _redis_client

async def get_proxy():
    """å¾ Redis é¸æ“‡æœ‰æ•ˆä»£ç†"""
    client = await get_redis_client()
    try:
        proxies_bytes = await client.smembers("proxy_pool")
        if not proxies_bytes:
            print("ç„¡å¯ç”¨ä»£ç†ï¼Œä½¿ç”¨ç›´æ¥é€£ç·š")
            return None
        proxies = [p.decode('utf-8') for p in proxies_bytes]
        random.shuffle(proxies)
        for proxy in proxies:
            try:
                async with httpx.AsyncClient(proxies={'http://': proxy, 'https://': proxy}, timeout=5) as test_client:
                    response = await test_client.get("http://httpbin.org/ip")
                    response.raise_for_status()
                print(f"ä½¿ç”¨ä»£ç†: {proxy}")
                return proxy
            except (httpx.RequestError, httpx.HTTPStatusError) as e:
                print(f"ä»£ç† {proxy} å¤±æ•—: {e}")
                await client.srem("proxy_pool", proxy)
        print("ç„¡æœ‰æ•ˆä»£ç†")
        return None
    except Exception as e:
        print(f"ç²å–ä»£ç†å¤±æ•—: {e}")
        return None

async def perform_crawl(url: str, task_id: str) -> str:
    """åŸ·è¡Œç¶²é çˆ¬å–"""
    user_agent = UserAgent().random
    proxy = await get_proxy()
    await asyncio.sleep(random.uniform(0.1, 1.0))  # éš¨æ©Ÿå»¶é²

    headers = {
        'User-Agent': user_agent,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.0,image/webp,*/*;q=0.8',
    }
    
    async with httpx.AsyncClient(headers=headers, proxies={'http://': proxy, 'https://': proxy} if proxy else None, 
                                 timeout=30, follow_redirects=True) as client:
        try:
            print(f"çˆ¬å–: {url} (ä»»å‹™ ID: {task_id})")
            response = await client.get(url)
            response.raise_for_status()
            redis_client = await get_redis_client()
            await redis_client.sadd("crawled_urls", url)  # URL å»é‡
            return response.text
        except httpx.HTTPStatusError as e:
            print(f"çˆ¬å–å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ {e.response.status_code}: {e}")
            if e.response.status_code in [403, 429]:
                print("å¯èƒ½éœ€è¦ CAPTCHA è§£æ±º")
            raise
```

**èªªæ˜**ï¼šå¯¦ç¾éåŒæ­¥çˆ¬å–ï¼Œæ”¯æ´ä»£ç†å’Œ User-Agent éš¨æ©ŸåŒ–ï¼Œé ç•™ CAPTCHA è§£æ±ºé‚è¼¯ã€‚

---

### 3. Celery å·¥ä½œç¯€é» - è³‡æ–™å„²å­˜ (`celery_workers/app/tasks.py`)

```python
from celery_app import app
import asyncio
import asyncpg
import datetime
import os
import redis.asyncio as aioredis

POSTGRES_URL = os.getenv("POSTGRES_URL", "postgresql://user:password@pgbouncer:6432/crawler_db")
REDIS_HOST = os.getenv("REDIS_HOST", "redis")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))

async def get_redis_client():
    """ç²å– Redis é€£ç·š"""
    return await aioredis.from_url(f"redis://{REDIS_HOST}:{REDIS_PORT}")

async def get_pg_pool():
    """ç²å– PostgreSQL é€£ç·šæ± """
    global _pg_pool
    if _pg_pool is None:
        if POSTGRES_URL:
            _pg_pool = await asyncpg.create_pool(POSTGRES_URL, min_size=10, max_size=50)
        else:
            raise ValueError("æœªè¨­ç½® POSTGRES_URL")
    return _pg_pool

@app.task(name='tasks.store_data', bind=True, default_retry_delay=300, max_retries=5)
def store_data_task(self, url: str, content: str, task_id: str = None):
    """å„²å­˜çˆ¬å–æ•¸æ“š"""
    print(f"è™•ç† URL: {url} (ä»»å‹™ ID: {task_id})")
    redis_client_sync = asyncio.run(get_redis_client())
    try:
        data_to_store = [{
            "url": url,
            "content": content,
            "crawled_at": datetime.datetime.now(),
            "task_id": task_id,
            "title": "æ¨™é¡Œä½”ä½ç¬¦",
            "metadata": {}
        }]

        asyncio.run(_store_data_async(data_to_store))
        
        asyncio.run(redis_client_sync.hset(f"crawl_task_status:{task_id}", mapping={
            "status": "completed",
            "completed_at": datetime.datetime.now().isoformat()
        }))
        print(f"å„²å­˜æˆåŠŸ: {url}")
        return f"å„²å­˜æˆåŠŸ: {url}"
    except Exception as e:
        print(f"å„²å­˜å¤±æ•—: {url}, éŒ¯èª¤: {e}")
        asyncio.run(redis_client_sync.hset(f"crawl_task_status:{task_id}", mapping={
            "status": "failed",
            "error": str(e),
            "completed_at": datetime.datetime.now().isoformat()
        }))
        raise self.retry(exc=e)
    finally:
        asyncio.run(redis_client_sync.close())

async def _store_data_async(data_items: list):
    """éåŒæ­¥å„²å­˜è‡³ PostgreSQL"""
    if POSTGRES_URL:
        pool = await get_pg_pool()
        async with pool.acquire() as conn:
            await conn.executemany('''
                INSERT INTO crawled_data(url, content, crawled_at, title, metadata, task_id)
                VALUES($1, $2, $3, $4, $5::jsonb, $6)
                ON CONFLICT (url) DO UPDATE SET
                    content = EXCLUDED.content,
                    crawled_at = EXCLUDED.crawled_at,
                    title = EXCLUDED.title,
                    metadata = EXCLUDED.metadata,
                    updated_at = NOW();
            ''', [(item['url'], item['content'], item['crawled_at'], item.get('title', ''), item.get('metadata', {}), item['task_id']) for item in data_items])
        print(f"æ‰¹é‡æ’å…¥ {len(data_items)} æ¢æ•¸æ“š")
```

**èªªæ˜**ï¼šè™•ç†è³‡æ–™å„²å­˜ï¼Œä½¿ç”¨ PostgreSQL é€£ç·šæ± ï¼Œæ›´æ–° Redis ä»»å‹™ç‹€æ…‹ã€‚

---

## ğŸ“Š ç›£æ§èˆ‡é™¤éŒ¯

- **Prometheus**ï¼š`http://localhost:9090`ï¼Œç›£æ§ QPS å’Œä»£ç†æ± å¤§å°ã€‚
- **Grafana**ï¼š`http://localhost:3000`ï¼ˆå¸³è™Ÿ/å¯†ç¢¼ï¼š`admin/admin`ï¼‰ï¼Œå¯è¦–åŒ–å„€è¡¨æ¿ã€‚
- **Celery Flower**ï¼š`http://localhost:5555`ï¼Œç›£æ§ä»»å‹™ç‹€æ…‹ã€‚
- **ELK Stack**ï¼ˆå¯é¸ï¼‰ï¼šå–æ¶ˆ `docker-compose.yml` ä¸­ç›¸é—œæœå‹™çš„è¨»é‡‹ï¼Œè¨ªå• `http://localhost:5601`ã€‚

---

## âš™ï¸ æ€§èƒ½èª¿å„ª

ç‚ºå¯¦ç¾ **10,000 QPS**ï¼š

1. **æ“´å±•æœå‹™**ï¼š
   - FastAPI å·¥ä½œé€²ç¨‹ï¼š
     ```yaml
     command: gunicorn -k uvicorn.workers.UvicornWorker -w 16 main:app_rest --bind 0.0.0.0:8000
     ```
   - Celery ä¸¦ç™¼åº¦ï¼š
     ```yaml
     command: celery -A celery_app worker --loglevel=info --concurrency=500 -P gevent
     ```

2. **ä»£ç†æ± **ï¼š
   - é…ç½®æœ‰æ•ˆ `PROXY_API_URL`ï¼ˆå¦‚ 98IPï¼‰ï¼Œç¢ºä¿ 100-200 å€‹ä»£ç†ã€‚
   - ç¸®çŸ­ `PROXY_UPDATE_INTERVAL_SECONDS` è‡³ 1800 ç§’ã€‚

3. **å£“åŠ›æ¸¬è©¦**ï¼š
   - ä½¿ç”¨ Locustï¼š
     ```python
     # locustfile.py
     from locust import HttpUser, task, between

     class CrawlerUser(HttpUser):
         wait_time = between(0.1, 0.5)
         host = "http://localhost:8000"

         @task
         def submit_crawl(self):
             self.client.post("/api/submit-crawl", json={"url": "http://example.com"})
     ```
   - åŸ·è¡Œï¼š`locust -f locustfile.py`

---

## ğŸ” åçˆ¬èŸ²æªæ–½

- **ä»£ç†ç®¡ç†**ï¼š`fastapi_crawler/app/proxy_manager.py` æ›´æ–° Redis ä»£ç†æ± ã€‚
- **User-Agent éš¨æ©ŸåŒ–**ï¼šä½¿ç”¨ `fake-useragent`ã€‚
- **CAPTCHA è§£æ±º**ï¼šæ”¯æ´ 2Captchaï¼Œéœ€å¯¦ç¾åœ–ç‰‡è§£æã€‚
- **æ“´å±•å»ºè­°**ï¼šæ·»åŠ  Playwrightï¼š
  ```yaml
  playwright:
    image: mcr.microsoft.com/playwright:v1.39.0
    networks:
      - crawler_network
  ```

---

## ğŸ“‚ å€‰åº«çµæ§‹

```plaintext
high_qps_crawler/
â”œâ”€â”€ laravel_api/               # Laravel API æ ¸å¿ƒç¨‹å¼ç¢¼
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ Http/Controllers/CrawlerController.php
â”‚   â”‚   â”œâ”€â”€ Grpc/CrawlerServiceClient.php
â”‚   â”œâ”€â”€ proto/crawler.proto
â”‚   â”œâ”€â”€ routes/api.php
â”‚   â”œâ”€â”€ database/migrations/
â”‚   â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ fastapi_crawler/           # FastAPI çˆ¬èŸ²æ ¸å¿ƒ
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ crawler.py
â”‚   â”‚   â”œâ”€â”€ grpc_server.py
â”‚   â”‚   â”œâ”€â”€ proxy_manager.py
â”‚   â”œâ”€â”€ proto/crawler.proto
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ celery_workers/            # Celery å·¥ä½œç¯€é»
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ tasks.py
â”‚   â”‚   â”œâ”€â”€ celery_app.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ monitoring/                # ç›£æ§é…ç½®
â”‚   â”œâ”€â”€ prometheus.yml
â”‚   â””â”€â”€ logstash.conf
â”œâ”€â”€ redis_data/                # Redis æŒä¹…åŒ–
â”œâ”€â”€ data_storage/              # è³‡æ–™åº«æŒä¹…åŒ–
â”‚   â”œâ”€â”€ mongodb_data/
â”‚   â””â”€â”€ postgresql_data/
â””â”€â”€ docker-compose.yml         # Docker Compose é…ç½®
```

---

## ğŸ¤ è²¢ç»æŒ‡å—

1. Fork å€‰åº«ã€‚
2. å‰µå»ºåˆ†æ”¯ï¼š`git checkout -b feature/ä½ çš„åŠŸèƒ½`ã€‚
3. æäº¤è®Šæ›´ï¼š`git commit -m 'æ·»åŠ ä½ çš„åŠŸèƒ½'`ã€‚
4. æ¨é€åˆ†æ”¯ï¼š`git push origin feature/ä½ çš„åŠŸèƒ½`ã€‚
5. é–‹å•Ÿ Pull Requestã€‚

