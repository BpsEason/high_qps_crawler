# é«˜ QPS ç¶²è·¯çˆ¬èŸ²ç³»çµ±

é€™æ˜¯ä¸€å€‹å°ˆç‚ºå¤§è¦æ¨¡ç¶²é æŠ“å–è¨­è¨ˆçš„ **é«˜æ€§èƒ½åˆ†æ•£å¼çˆ¬èŸ²ç³»çµ±**ï¼Œç›®æ¨™å¯¦ç¾ **æ¯ç§’ 10,000 æ¬¡æŸ¥è©¢ (QPS)**ï¼Œå…¼é¡§ç©©å®šæ€§ã€å¯æ“´å±•æ€§å’Œåçˆ¬èŸ²èƒ½åŠ›ã€‚æœ¬å€‰åº«æä¾›æ ¸å¿ƒç¨‹å¼ç¢¼ï¼ŒåŸºæ–¼ **Laravel**ã€**FastAPI**ã€**gRPC**ã€**Redis**ã€**Celery** å’Œ **PostgreSQL/MongoDB** æ‰“é€ ï¼Œå±•ç¾æ¨¡çµ„åŒ–è¨­è¨ˆã€é«˜æ•ˆé€šä¿¡å’Œç”Ÿç”¢ç´šéƒ¨ç½²èƒ½åŠ›ã€‚ç³»çµ±æ•´åˆäº† **Prometheus**ã€**Grafana** å’Œ **Celery Flower** ç›£æ§ï¼Œç¢ºä¿æ€§èƒ½é€æ˜ã€‚

**äº®é»**ï¼š
- **æ¥µè‡´æ€§èƒ½**ï¼šå–®ç¯€é»æ”¯æ´ 1,600-4,000 QPSï¼Œæ°´å¹³æ“´å±•å¯é” 10,000 QPSã€‚
- **åçˆ¬èŸ²**ï¼šå‹•æ…‹ä»£ç†æ± ã€éš¨æ©Ÿ User-Agent å’Œ 2Captcha æ”¯æ´ï¼Œè¦é¿å°é–ã€‚
- **åˆ†æ•£å¼æ¶æ§‹**ï¼šgRPC é«˜æ•ˆé€šä¿¡ï¼ŒRedis ä»»å‹™ä½‡åˆ—ï¼ŒCelery é«˜ä¸¦ç™¼è™•ç†ã€‚
- **ç›£æ§èˆ‡èª¿å„ª**ï¼šå¯¦æ™‚æŒ‡æ¨™å¯è¦–åŒ–ï¼Œæ”¯æ´å£“åŠ›æ¸¬è©¦èˆ‡æ•ˆèƒ½å„ªåŒ–ã€‚

**æ³¨æ„**ï¼šæœ¬å€‰åº«åƒ…å«æ ¸å¿ƒç¨‹å¼ç¢¼ï¼ŒLaravel åŸºç¤ç’°å¢ƒéœ€è‡ªè¡Œå®‰è£ã€‚

---

## ğŸ› ï¸ ç³»çµ±æ¶æ§‹

ç³»çµ±æ¡ç”¨æ¨¡çµ„åŒ–åˆ†æ•£å¼è¨­è¨ˆï¼Œæ ¸å¿ƒçµ„ä»¶å”åŒå¯¦ç¾é«˜ QPSï¼š

1. **Laravel API** (`laravel_api/`)ï¼š
   - æä¾› REST ç«¯é» (`/api/submit-crawl`, `/api/crawl-status/{task_id}`) ç®¡ç†ä»»å‹™ã€‚
   - ä½¿ç”¨ **gRPC** èˆ‡ FastAPI é€šä¿¡ï¼Œæ•ˆç‡é«˜æ–¼ RESTã€‚
   - ä»»å‹™ç‹€æ…‹å­˜æ–¼ **Redis**ï¼Œæ­·å²è³‡æ–™é€² **PostgreSQL** æˆ– **MongoDB**ã€‚

2. **FastAPI çˆ¬èŸ²æ ¸å¿ƒ** (`fastapi_crawler/`)ï¼š
   - åŸºæ–¼ `aiohttp` å’Œ `httpx` çš„éåŒæ­¥ HTTP è«‹æ±‚ï¼Œæ”¯æ´é«˜ä¸¦ç™¼çˆ¬å–ã€‚
   - å¯¦ç¾ä»£ç†æ± ç®¡ç†å’Œåçˆ¬èŸ²ç­–ç•¥ï¼ˆUser-Agent éš¨æ©ŸåŒ–ã€éš¨æ©Ÿå»¶é²ï¼‰ã€‚
   - æä¾› gRPC æœå‹™ (`app/grpc_server.py`) å’Œ REST å¥åº·æª¢æŸ¥ç«¯é» (`main.py`)ã€‚

3. **Celery å·¥ä½œç¯€é»** (`celery_workers/`)ï¼š
   - ä½¿ç”¨ `gevent` æ¨¡å¼ï¼Œæ¯ç¯€é»æ”¯æ´ 300 ä¸¦ç™¼ä»»å‹™ï¼Œè™•ç†è³‡æ–™å„²å­˜ã€‚
   - æ”¯æ´ **PostgreSQL**ï¼ˆæ­é… PgBouncer é€£ç·šæ± ï¼‰æˆ– **MongoDB**ã€‚

4. **Redis**ï¼š
   - ä½œç‚ºä»»å‹™ä½‡åˆ—ï¼ˆCeleryï¼‰ã€ä»£ç†æ± å’Œ URL å»é‡å„²å­˜ã€‚
   - é…ç½®ä¸»å¾è¤‡è£½ï¼Œç¢ºä¿é«˜å¯ç”¨æ€§ã€‚

5. **ç›£æ§ç³»çµ±**ï¼š
   - **Prometheus**ï¼šæ”¶é›† QPSã€å»¶é²ã€ä»£ç†æ± å¤§å°ç­‰æŒ‡æ¨™ã€‚
   - **Grafana**ï¼šå¯è¦–åŒ–æ•ˆèƒ½å„€è¡¨æ¿ã€‚
   - **Celery Flower**ï¼šå³æ™‚ç›£æ§ä»»å‹™åŸ·è¡Œã€‚
   - **ELK Stack**ï¼ˆå¯é¸ï¼‰ï¼šé›†ä¸­å¼æ—¥èªŒåˆ†æã€‚

---

## ğŸ“‹ ç’°å¢ƒéœ€æ±‚

- **Docker** å’Œ **Docker Compose**ï¼šå®¹å™¨åŒ–éƒ¨ç½²ã€‚
- **Laravel ç’°å¢ƒ**ï¼šPHP 8.2+ã€Composerï¼ˆéœ€è‡ªè¡Œå®‰è£ï¼‰ã€‚
- **ç¡¬é«”å»ºè­°**ï¼ˆ10,000 QPSï¼‰ï¼š
  - CPUï¼š32-64 æ ¸å¿ƒï¼ˆä¾‹ï¼šAWS c6i.16xlargeï¼‰ã€‚
  - è¨˜æ†¶é«”ï¼š128-256 GBã€‚
  - ç¶²è·¯ï¼š10 Gbpsã€‚
  - å„²å­˜ï¼šNVMe SSDï¼ˆ1-2 TBï¼‰ã€‚
- **ä»£ç† API**ï¼šå¯é æä¾›å•†ï¼ˆå¦‚ 98IPã€BrightDataï¼‰ã€‚
- **2Captcha API é‡‘é‘°**ï¼ˆå¯é¸ï¼‰ï¼šè§£ CAPTCHAã€‚

---

## ğŸ—ï¸ å¿«é€Ÿéƒ¨ç½²

1. **å…‹éš†ç¨‹å¼ç¢¼**ï¼š
   ```bash
   git clone https://github.com/BpsEason/high_qps_crawler.git
   cd high_qps_crawler
   ```

2. **å®‰è£ Laravel ç’°å¢ƒ**ï¼š
   - é€²å…¥ `laravel_api/`ï¼š
     ```bash
     cd laravel_api
     composer install
     cp .env.example .env
     php artisan key:generate
     ```

3. **é…ç½®ç’°å¢ƒè®Šæ•¸**ï¼š
   - `fastapi_crawler/.env`ï¼š
     ```env
     CAPTCHA_API_KEY=ä½ çš„_2captcha_api_é‡‘é‘°  # å¯é¸
     PROXY_API_URL=https://api.your-proxy-provider.com/get-proxies  # çœŸå¯¦ä»£ç† API
     PROXY_UPDATE_INTERVAL_SECONDS=3600
     ```
   - `laravel_api/.env`ï¼š
     ```env
     DB_CONNECTION=pgsql
     DB_HOST=pgbouncer
     DB_PORT=6432
     DB_DATABASE=crawler_db
     DB_USERNAME=user
     DB_PASSWORD=password
     REDIS_HOST=redis
     REDIS_PORT=6379
     FASTAPI_GRPC_HOST=fastapi_crawler:50051
     ```

4. **é¸æ“‡è³‡æ–™åº«**ï¼š
   - é è¨­ï¼š**PostgreSQL**ï¼ˆæ­é… PgBouncerï¼‰ã€‚
   - ç”¨ **MongoDB**ï¼šæ”¹ `docker-compose.yml` å’Œ `laravel_api/.env`ï¼š
     ```env
     DB_CONNECTION=mongodb
     DB_HOST=mongodb
     DB_PORT=27017
     DB_DATABASE=crawler_db
     MONGODB_URL=mongodb://mongodb:27017
     ```

5. **å•Ÿå‹•æœå‹™**ï¼š
   ```bash
   docker compose build
   docker compose up -d
   ```

6. **åˆå§‹åŒ–è³‡æ–™åº«**ï¼š
   ```bash
   docker compose exec laravel php artisan migrate
   ```

7. **æ“´å±• Celery**ï¼š
   ```bash
   docker compose up -d --scale celery_worker=10
   ```

8. **æ¸¬è©¦çˆ¬èŸ²**ï¼š
   - æäº¤ä»»å‹™ï¼š
     ```bash
     curl -X POST -H "Content-Type: application/json" -d '{"url": "http://example.com"}' http://localhost:8000/api/submit-crawl
     ```
   - æŸ¥ç‹€æ…‹ï¼š
     ```bash
     curl http://localhost:8000/api/crawl-status/{task_id}
     ```

---

## ğŸ”‘ æ ¸å¿ƒç¨‹å¼ç¢¼èˆ‡æŠ€è¡“äº®é»

ä»¥ä¸‹å±•ç¤ºæ ¸å¿ƒç¨‹å¼ç¢¼ï¼Œæ­é…ä¸­æ–‡è¨»è§£ï¼Œçªå‡ºæŠ€è¡“å«é‡ï¼š

### 1. Laravel API - é«˜æ•ˆä»»å‹™åˆ†æ´¾ (`laravel_api/app/Http/Controllers/CrawlerController.php`)

```php
<?php
namespace App\Http\Controllers;

use Illuminate\Http\Request;
use Grpc\ChannelCredentials;
use App\Grpc\CrawlerServiceClient;
use Illuminate\Support\Facades\Log;
use Illuminate\Support\Facades\Redis;

class CrawlerController extends Controller
{
    /**
     * æäº¤çˆ¬å–ä»»å‹™ï¼Œé€é gRPC èˆ‡ FastAPI é«˜æ•ˆé€šä¿¡
     */
    public function submitCrawlTask(Request $request)
    {
        // é©—è­‰ URL æ ¼å¼ï¼Œç¢ºä¿è¼¸å…¥å®‰å…¨
        $request->validate(['url' => 'required|url']);
        $url = $request->input('url');
        $taskId = uniqid('crawl_'); // ç”Ÿæˆå”¯ä¸€ä»»å‹™ IDï¼Œæ”¯æ´é«˜ä¸¦ç™¼

        try {
            // åˆå§‹åŒ– gRPC å®¢æˆ¶ç«¯ï¼Œä½¿ç”¨ HTTP/2 é«˜æ•ˆå‚³è¼¸
            $client = new CrawlerServiceClient(env('FASTAPI_GRPC_HOST', 'fastapi_crawler:50051'), [
                'credentials' => ChannelCredentials::createInsecure(),
            ]);
            
            // æ§‹å»º gRPC è«‹æ±‚ï¼ŒProtocol Buffers ç¢ºä¿è³‡æ–™çµæ§‹åš´è¬¹
            $requestProto = new \Crawler\CrawlRequest();
            $requestProto->setUrl($url);
            $requestProto->setTaskId($taskId);

            // ç™¼é€ gRPC è«‹æ±‚ï¼Œä½å»¶é²é«˜åå
            list($response, $status) = $client->Crawl($requestProto)->wait();
            if ($status->code !== \Grpc\STATUS_OK) {
                throw new \Exception("gRPC é€šä¿¡å¤±æ•—: " . $status->details);
            }

            // å„²å­˜ä»»å‹™ç‹€æ…‹è‡³ Redisï¼Œæ”¯æ´å³æ™‚æŸ¥è©¢èˆ‡é«˜ä¸¦ç™¼
            Redis::hset("crawl_task_status:{$taskId}", "status", "dispatched");
            Redis::hset("crawl_task_status:{$taskId}", "url", $url);
            Redis::hset("crawl_task_status:{$taskId}", "dispatched_at", now()->toIso8601String());

            Log::info("ä»»å‹™åˆ†æ´¾æˆåŠŸï¼ŒURL: $url, ä»»å‹™ ID: $taskId");
            return response()->json([
                'message' => 'ä»»å‹™å·²åˆ†æ´¾è‡³çˆ¬èŸ²æ ¸å¿ƒ',
                'task_id' => $taskId,
                'grpc_status' => $response->getStatus(),
            ]);
        } catch (\Exception $e) {
            // ç•°å¸¸è™•ç†ï¼Œè¨˜éŒ„éŒ¯èª¤ä¸¦æ›´æ–° Redis ç‹€æ…‹
            Log::error("ä»»å‹™åˆ†æ´¾å¤±æ•—ï¼ŒURL: $url, éŒ¯èª¤: " . $e->getMessage());
            Redis::hset("crawl_task_status:{$taskId}", "status", "submission_failed");
            Redis::hset("crawl_task_status:{$taskId}", "error", $e->getMessage());
            return response()->json(['error' => 'åˆ†æ´¾å¤±æ•—: ' . $e->getMessage()], 500);
        }
    }
}
```

**æŠ€è¡“äº®é»**ï¼š
- **gRPC é€šä¿¡**ï¼šæ¯” REST å¿« 2-10 å€ï¼Œé©åˆé«˜é »ä»»å‹™åˆ†æ´¾ã€‚
- **Redis ç‹€æ…‹ç®¡ç†**ï¼šç”¨ Hash çµæ§‹å­˜ä»»å‹™ç‹€æ…‹ï¼Œæ”¯æ´ O(1) æŸ¥è©¢ã€‚
- **ç•°å¸¸è™•ç†**ï¼šç¢ºä¿ç³»çµ±ç©©å®šï¼ŒéŒ¯èª¤å³æ™‚è¨˜éŒ„ã€‚

---

### 2. FastAPI çˆ¬èŸ²æ ¸å¿ƒ - éåŒæ­¥é«˜ä¸¦ç™¼çˆ¬å– (`fastapi_crawler/app/crawler.py`)

```python
import aiohttp
import httpx
from fake_useragent import UserAgent
import asyncio
import redis.asyncio as aioredis
import random
import os

REDIS_HOST = os.getenv("REDIS_HOST", "redis")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))

async def get_redis_client():
    """åˆå§‹åŒ– Redis é€£ç·šï¼Œæ”¯æ´ä»£ç†æ± å’Œ URL å»é‡"""
    global _redis_client
    if _redis_client is None:
        _redis_client = await aioredis.from_url(f"redis://{REDIS_HOST}:{REDIS_PORT}")
    return _redis_client

async def get_proxy():
    """å¾ Redis å‹•æ…‹ä»£ç†æ± é¸å–å¥åº·ä»£ç†ï¼Œç¢ºä¿é«˜å¯ç”¨æ€§"""
    client = await get_redis_client()
    try:
        proxies_bytes = await client.smembers("proxy_pool")
        if not proxies_bytes:
            print("ä»£ç†æ± ç©ºï¼Œæ”¹ç”¨ç›´é€£")
            return None
        proxies = [p.decode('utf-8') for p in proxies_bytes]
        random.shuffle(proxies)  # éš¨æ©Ÿé¸æ“‡ï¼Œåˆ†æ•£è«‹æ±‚å£“åŠ›
        for proxy in proxies:
            try:
                async with httpx.AsyncClient(proxies={'http://': proxy, 'https://': proxy}, timeout=5) as test_client:
                    response = await test_client.get("http://httpbin.org/ip")
                    response.raise_for_status()
                print(f"é¸ç”¨ä»£ç†: {proxy}")
                return proxy
            except (httpx.RequestError, httpx.HTTPStatusError) as e:
                print(f"ä»£ç† {proxy} ç„¡æ•ˆ: {e}ï¼Œç§»é™¤")
                await client.srem("proxy_pool", proxy)
        print("ç„¡æœ‰æ•ˆä»£ç†ï¼Œæ”¹ç”¨ç›´é€£")
        return None
    except Exception as e:
        print(f"ç²å–ä»£ç†å¤±æ•—: {e}")
        return None

async def perform_crawl(url: str, task_id: str) -> str:
    """éåŒæ­¥çˆ¬å–ç¶²é ï¼Œæ”¯æ´åçˆ¬èŸ²ç­–ç•¥"""
    user_agent = UserAgent().random  # éš¨æ©Ÿ User-Agentï¼Œé™ä½å°é–é¢¨éšª
    proxy = await get_proxy()  # å‹•æ…‹é¸æ“‡ä»£ç†
    await asyncio.sleep(random.uniform(0.1, 1.0))  # éš¨æ©Ÿå»¶é²ï¼Œæ¨¡æ“¬äººé¡è¡Œç‚º

    headers = {
        'User-Agent': user_agent,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    }
    
    async with httpx.AsyncClient(headers=headers, proxies={'http://': proxy, 'https://': proxy} if proxy else None, 
                                 timeout=30, follow_redirects=True) as client:
        try:
            print(f"é–‹å§‹çˆ¬å–: {url} (ä»»å‹™ ID: {task_id})")
            response = await client.get(url)
            response.raise_for_status()
            redis_client = await get_redis_client()
            await redis_client.sadd("crawled_urls", url)  # URL å»é‡ï¼Œé˜²æ­¢é‡è¤‡çˆ¬å–
            return response.text
        except httpx.HTTPStatusError as e:
            print(f"çˆ¬å–å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ {e.response.status_code}: {e}")
            if e.response.status_code in [403, 429]:
                print("æª¢æ¸¬åˆ°åçˆ¬ï¼Œå¯èƒ½éœ€ CAPTCHA è§£æ±º")
                # é ç•™ 2Captcha è™•ç†é‚è¼¯
            raise
```

**æŠ€è¡“äº®é»**ï¼š
- **éåŒæ­¥ I/O**ï¼š`httpx.AsyncClient` æ”¯æ´é«˜ä¸¦ç™¼ï¼Œå–®ç¯€é»å¯è™•ç†æ•¸åƒ QPSã€‚
- **å‹•æ…‹ä»£ç†æ± **ï¼šRedis å„²å­˜ä¸¦æª¢æŸ¥ä»£ç†å¥åº·ï¼Œè‡ªå‹•å‰”é™¤ç„¡æ•ˆä»£ç†ã€‚
- **åçˆ¬ç­–ç•¥**ï¼šéš¨æ©Ÿå»¶é² (0.1-1.0 ç§’)ã€User-Agent éš¨æ©ŸåŒ–ï¼Œé ç•™ CAPTCHA è™•ç†ã€‚

---

### 3. Celery å·¥ä½œç¯€é» - é«˜ä¸¦ç™¼è³‡æ–™å„²å­˜ (`celery_workers/app/tasks.py`)

```python
from celery_app import app
import asyncio
import asyncpg
import datetime
import os
import redis.asyncio as aioredis

POSTGRES_URL = os.getenv("POSTGRES_URL", "postgresql://user:password@pgbouncer:6432/crawler_db")
REDIS_HOST = os.getenv("REDIS_HOST", "redis")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))

async def get_redis_client():
    """åˆå§‹åŒ– Redis é€£ç·šï¼Œæ›´æ–°ä»»å‹™ç‹€æ…‹"""
    return await aioredis.from_url(f"redis://{REDIS_HOST}:{REDIS_PORT}")

async def get_pg_pool():
    """å»ºç«‹ PostgreSQL é€£ç·šæ± ï¼Œæ”¯æ´é«˜ä¸¦ç™¼å¯«å…¥"""
    global _pg_pool
    if _pg_pool is None:
        if POSTGRES_URL:
            _pg_pool = await asyncpg.create_pool(POSTGRES_URL, min_size=10, max_size=50)
        else:
            raise ValueError("æœªè¨­ç½® POSTGRES_URL")
    return _pg_pool

@app.task(name='tasks.store_data', bind=True, default_retry_delay=300, max_retries=5)
def store_data_task(self, url: str, content: str, task_id: str = None):
    """é«˜æ•ˆå„²å­˜çˆ¬å–è³‡æ–™ï¼Œæ”¯æ´å¤±æ•—é‡è©¦"""
    print(f"è™•ç† URL: {url} (ä»»å‹™ ID: {task_id})")
    redis_client_sync = asyncio.run(get_redis_client())
    try:
        data_to_store = [{
            "url": url,
            "content": content,
            "crawled_at": datetime.datetime.now(),
            "task_id": task_id,
            "title": "æ¨™é¡Œä½”ä½",
            "metadata": {}
        }]

        # æ‰¹é‡å„²å­˜ï¼Œæ¸›å°‘è³‡æ–™åº«å£“åŠ›
        asyncio.run(_store_data_async(data_to_store))
        
        # æ›´æ–° Redis ä»»å‹™ç‹€æ…‹
        asyncio.run(redis_client_sync.hset(f"crawl_task_status:{task_id}", mapping={
            "status": "completed",
            "completed_at": datetime.datetime.now().isoformat()
        }))
        print(f"å„²å­˜æˆåŠŸ: {url}")
        return f"å„²å­˜æˆåŠŸ: {url}"
    except Exception as e:
        print(f"å„²å­˜å¤±æ•—: {url}, éŒ¯èª¤: {e}")
        # è¨˜éŒ„å¤±æ•—ç‹€æ…‹ï¼Œæ”¯æ´é™¤éŒ¯
        asyncio.run(redis_client_sync.hset(f"crawl_task_status:{task_id}", mapping={
            "status": "failed",
            "error": str(e),
            "completed_at": datetime.datetime.now().isoformat()
        }))
        raise self.retry(exc=e)  # è‡ªå‹•é‡è©¦ï¼Œæœ€å¤š 5 æ¬¡
    finally:
        asyncio.run(redis_client_sync.close())

async def _store_data_async(data_items: list):
    """éåŒæ­¥æ‰¹é‡å¯«å…¥ PostgreSQLï¼Œå„ªåŒ–æ€§èƒ½"""
    pool = await get_pg_pool()
    async with pool.acquire() as conn:
        await conn.executemany('''
            INSERT INTO crawled_data(url, content, crawled_at, title, metadata, task_id)
            VALUES($1, $2, $3, $4, $5::jsonb, $6)
            ON CONFLICT (url) DO UPDATE
            SET content = EXCLUDED.content, crawled_at = EXCLUDED.crawled_at,
                title = EXCLUDED.title, metadata = EXCLUDED.metadata,
                updated_at = NOW();
        ''', [(item['url'], item['content'], item['crawled_at'], item.get('title', ''),
               item.get('metadata', {}), item['task_id']) for item in data_items])
    print(f"æ‰¹é‡å¯«å…¥ {len(data_items)} æ¢è³‡æ–™")
```

**æŠ€è¡“äº®é»**ï¼š
- **é€£ç·šæ± **ï¼š`asyncpg` æ­é… PgBouncerï¼Œæ”¯æ´é«˜ä¸¦ç™¼å¯«å…¥ï¼ˆ10-50 é€£ç·šï¼‰ã€‚
- **æ‰¹é‡æ’å…¥**ï¼š`executemany` æ¸›å°‘ SQL åŸ·è¡Œæ¬¡æ•¸ï¼Œæå‡æ•ˆç‡ã€‚
- **å¤±æ•—é‡è©¦**ï¼šCelery è‡ªå‹•é‡è©¦ï¼Œæ­é… Redis ç‹€æ…‹è¿½è¹¤ï¼Œç¢ºä¿ç©©å®šæ€§ã€‚

---

## â“ å¸¸è¦‹å•é¡Œèˆ‡è§£ç­”

ä»¥ä¸‹æ˜¯é–‹ç™¼è€…å¯èƒ½é—œå¿ƒçš„å•é¡Œï¼Œæ­é…ç¨‹å¼ç¢¼å’ŒæŠ€è¡“ç´°ç¯€ï¼Œè®“ä½ å¿«é€ŸæŒæ¡å°ˆæ¡ˆç²¾é«“ã€‚

### Q1: æ€éº¼è®“é€™ç³»çµ±è·‘åˆ° 10,000 QPSï¼Ÿ

é€™ç³»çµ±ç”¨åˆ†æ•£å¼æ¶æ§‹ï¼Œé  **Laravel** åˆ†æ´¾ä»»å‹™ã€**FastAPI** éåŒæ­¥çˆ¬å–ã€**Celery** è™•ç†è³‡æ–™ï¼Œæ­é… **Redis** ç®¡ç†ä½‡åˆ—å’Œä»£ç†ã€‚æ ¸å¿ƒè¨­è¨ˆï¼š

- **éåŒæ­¥çˆ¬å–**ï¼šFastAPI çš„ `perform_crawl` ç”¨ `httpx.AsyncClient`ï¼Œå–®ç¯€é»å¯é” 1,600-4,000 QPSã€‚
- **åˆ†æ•£å¼è™•ç†**ï¼šCelery ç”¨ `gevent` æ¨¡å¼ï¼Œæ¯ worker è·‘ 300 ä»»å‹™ï¼Œæ”¯æ´é«˜ä¸¦ç™¼ã€‚
- **æ°´å¹³æ“´å±•**ï¼šç”¨ `docker compose up -d --scale celery_worker=10` é–‹å¤šç¯€é»ã€‚

**ç¨‹å¼ç¢¼**ï¼ˆ`fastapi_crawler/app/crawler.py`ï¼‰ï¼š
```python
async def perform_crawl(url: str, task_id: str) -> str:
    user_agent = UserAgent().random
    proxy = await get_proxy()
    await asyncio.sleep(random.uniform(0.1, 1.0))
    headers = {'User-Agent': user_agent, ...}
    async with httpx.AsyncClient(headers=headers, proxies={'http://': proxy} if proxy else None,
                                 timeout=30, follow_redirects=True) as client:
        response = await client.get(url)
        response.raise_for_status()
        redis_client = await get_redis_client()
        await redis_client.sadd("crawled_urls", url)
        return response.text
```

**æ€éº¼è·‘æ›´å¿«**ï¼š
- ç”¨çœŸå¯¦ä»£ç† APIï¼ˆå¦‚ 98IPï¼‰ï¼Œä¿æŒ 100-200 å€‹å¥åº·ä»£ç†ã€‚
- èª¿ FastAPI workerï¼š`gunicorn -k uvicorn.workers.UvicornWorker -w 16`ã€‚
- ç”¨ Locust æ¸¬æ•ˆèƒ½ï¼š
  ```python
  # locustfile.py
  from locust import HttpUser, task, between
  class CrawlerUser(HttpUser):
      wait_time = between(0.1, 0.5)
      host = "http://localhost:8000"
      @task
      def submit_crawl(self):
          self.client.post("/api/submit-crawl", json={"url": "http://example.com"})
  ```
  ```bash
  locust -f locustfile.py
  ```

---

### Q2: è³‡æ–™åº«æ€éº¼æ’é«˜ä¸¦ç™¼ï¼Ÿ

ç”¨ **PostgreSQL** é… **PgBouncer** é€£ç·šæ± ï¼Œæ­é… `asyncpg` æ‰¹é‡å¯«å…¥ï¼Œæ¸›å°‘é€£ç·šé–‹éŠ·ã€‚å¦‚æœç”¨ **MongoDB**ï¼Œå‰‡é ç„¡æ¨¡å¼çµæ§‹è™•ç†éˆæ´»è³‡æ–™ã€‚

**ç¨‹å¼ç¢¼**ï¼ˆ`celery_workers/app/tasks.py`ï¼‰ï¼š
```python
async def _store_data_async(data_items: list):
    pool = await get_pg_pool()  # é€£ç·šæ± ï¼Œ10-50 é€£ç·š
    async with pool.acquire() as conn:
        await conn.executemany('''
            INSERT INTO crawled_data(url, content, crawled_at, title, metadata, task_id)
            VALUES($1, $2, $3, $4, $5::jsonb, $6)
            ON CONFLICT (url) DO UPDATE
            SET content = EXCLUDED.content, ...
        ''', [(item['url'], item['content'], item['crawled_at'], ...) for item in data_items])
    print(f"æ‰¹é‡å¯«å…¥ {len(data_items)} æ¢è³‡æ–™")
```

**å„ªåŒ–æŠ€å·§**ï¼š
- PgBouncer è¨­ `MAX_CLIENT_CONNECTIONS=1000`ï¼Œ`DEFAULT_POOL_SIZE=50`ã€‚
- ç”¨ `ON CONFLICT` è™•ç†å»é‡ï¼ŒçœæŸ¥è©¢ã€‚
- åœ¨ `url` æ¬„ä½åŠ å”¯ä¸€ç´¢å¼•ã€‚
- MongoDB å¯è€ƒæ…®åˆ†ç‰‡ï¼Œè™•ç†è¶…å¤§è³‡æ–™é‡ã€‚

---

### Q3: æ€éº¼æ‡‰å°ç¶²ç«™åçˆ¬èŸ²ï¼Ÿ

ç³»çµ±å…§å»ºä¸‰æ‹›ï¼š

1. **å‹•æ…‹ä»£ç†æ± **ï¼š`proxy_manager.py` å®šæœŸå¾ API æŠ“ä»£ç†ï¼Œå­˜ Redisï¼Œè‡ªå‹•å‰”é™¤ç„¡æ•ˆä»£ç†ã€‚
2. **éš¨æ©Ÿ User-Agent**ï¼šç”¨ `fake-useragent` æ¨¡æ“¬ä¸åŒç€è¦½å™¨ã€‚
3. **CAPTCHA æ”¯æ´**ï¼šé ç•™ 2Captcha è™•ç†ï¼Œé‡å° 403/429 éŒ¯èª¤ã€‚

**ç¨‹å¼ç¢¼**ï¼ˆ`fastapi_crawler/app/crawler.py`ï¼‰ï¼š
```python
async def get_proxy():
    client = await get_redis_client()
    proxies_bytes = await client.smembers("proxy_pool")
    proxies = [p.decode('utf-8') for p in proxies_bytes]
    random.shuffle(proxies)
    for proxy in proxies:
        try:
            async with httpx.AsyncClient(proxies={'http://': proxy, 'https://': proxy}, timeout=5) as test_client:
                response = await test_client.get("http://httpbin.org/ip")
                response.raise_for_status()
            return proxy
        except:
            await client.srem("proxy_pool", proxy)
    return None
```

**é€²éšå»ºè­°**ï¼š
- éš¨æ©Ÿå»¶é² 0.1-1.0 ç§’ï¼Œæ¨¡æ“¬äººé¡è¡Œç‚ºã€‚
- åŠ  Playwright è™•ç†å‹•æ…‹é é¢ï¼š
  ```yaml
  playwright:
    image: mcr.microsoft.com/playwright:v1.39.0
    networks:
      - crawler_network
  ```

---

### Q4: æ€éº¼çŸ¥é“ç³»çµ±è·‘å¾—å¥½ä¸å¥½ï¼Ÿ

å°ˆæ¡ˆæ•´åˆäº†å°ˆæ¥­ç›£æ§å·¥å…·ï¼š

- **Prometheus**ï¼š`http://localhost:9090`ï¼Œè¿½è¹¤ QPSã€å»¶é²ã€ä»£ç†æ•¸ã€‚
- **Grafana**ï¼š`http://localhost:3000`ï¼ˆ`admin/admin`ï¼‰ï¼Œç•«å‡ºæ•ˆèƒ½åœ–è¡¨ã€‚
- **Celery Flower**ï¼š`http://localhost:5555`ï¼Œçœ‹ä»»å‹™åŸ·è¡Œç‹€æ³ã€‚
- **ELK Stack**ï¼ˆå¯é¸ï¼‰ï¼šæ”¹ `docker-compose.yml`ï¼Œç”¨ Kibana (`http://localhost:5601`) æŸ¥æ—¥èªŒã€‚

**èª¿å„ªå»ºè­°**ï¼š
- èª¿ FastAPIï¼š`gunicorn -w 16`ã€‚
- èª¿ Celeryï¼š`--concurrency=500 -P gevent`ã€‚
- ç”¨ Locust æ‰¾ç“¶é ¸ã€‚

---

### Q5: ç‚ºå•¥ç”¨ gRPC é€£ Laravel å’Œ FastAPIï¼Ÿ

gRPC ç”¨ HTTP/2 å’Œ Protocol Buffersï¼Œæ¯” REST å¿« 2-10 å€ï¼Œé©åˆé«˜é »å…§éƒ¨é€šä¿¡ï¼š

- **é«˜æ•ˆ**ï¼šä½åºåˆ—åŒ–é–‹éŠ·ï¼Œæ”¯æ´é«˜ QPSã€‚
- **å‹åˆ¥å®‰å…¨**ï¼š`crawler.proto` å®šç¾©çµæ§‹ï¼Œæ¸›å°‘ bugã€‚
- **æœªä¾†æ“´å±•**ï¼šæ”¯æ´é›™å‘æµï¼Œå¯åŠ å³æ™‚ç‹€æ…‹å›é¥‹ã€‚

**ç¨‹å¼ç¢¼**ï¼ˆ`laravel_api/app/Grpc/CrawlerServiceClient.php`ï¼‰ï¼š
```php
$client = new CrawlerServiceClient(env('FASTAPI_GRPC_HOST', 'fastapi_crawler:50051'), [
    'credentials' => ChannelCredentials::createInsecure(),
]);
$requestProto = new \Crawler\CrawlRequest();
$requestProto->setUrl($url);
$requestProto->setTaskId($taskId);
list($response, $status) = $client->Crawl($requestProto)->wait();
```

**å•¥æ™‚ç”¨ REST**ï¼Ÿå¿«é€ŸåŸå‹æˆ–è·¨èªè¨€æ™‚ï¼ŒREST ç°¡å–®ï¼Œä½† gRPC æ›´é©åˆé«˜æ€§èƒ½å ´æ™¯ã€‚

---

### Q6: ä»»å‹™å¤±æ•—æ€éº¼è™•ç†ï¼Ÿ

ç”¨ **Celery** çš„é‡è©¦æ©Ÿåˆ¶å’Œ **Redis** ç‹€æ…‹è¿½è¹¤ï¼š

- **é‡è©¦**ï¼šä»»å‹™å¤±æ•—å¾Œç­‰ 300 ç§’é‡è©¦ï¼Œæœ€å¤š 5 æ¬¡ã€‚
- **ç‹€æ…‹ç®¡ç†**ï¼šå¤±æ•—è¨˜åˆ° Redisï¼Œæ–¹ä¾¿é™¤éŒ¯ã€‚

**ç¨‹å¼ç¢¼**ï¼ˆ`celery_workers/app/tasks.py`ï¼‰ï¼š
```python
@app.task(name='tasks.store_data', bind=True, default_retry_delay=300, max_retries=5)
def store_data_task(self, url: str, content: str, task_id: str = None):
    try:
        data_to_store = [{"url": url, "content": content, ...}]
        asyncio.run(_store_data_async(data_to_store))
        asyncio.run(redis_client_sync.hset(f"crawl_task_status:{task_id}", mapping={
            "status": "completed",
            "completed_at": datetime.datetime.now().isoformat()
        }))
    except Exception as e:
        asyncio.run(redis_client_sync.hset(f"crawl_task_status:{task_id}", mapping={
            "status": "failed",
            "error": str(e),
            ...
        }))
        raise self.retry(exc=e)
```

**å°è¨£ç«…**ï¼š
- èª¿æ•´ `default_retry_delay` é¿å…éåº¦é‡è©¦ã€‚
- Redis ç‹€æ…‹æ–¹ä¾¿æŸ¥å•é¡Œæ ¹å› ã€‚

---

## ğŸ“Š ç›£æ§èˆ‡é™¤éŒ¯

- **Prometheus**ï¼š`http://localhost:9090`ã€‚
- **Grafana**ï¼š`http://localhost:3000`ï¼ˆ`admin/admin`ï¼‰ã€‚
- **Celery Flower**ï¼š`http://localhost:5555`ã€‚
- **ELK Stack**ï¼ˆå¯é¸ï¼‰ï¼š`http://localhost:5601`ã€‚

---

## âš™ï¸ æ€§èƒ½å„ªåŒ–

æƒ³è¡ **10,000 QPS**ï¼š
- FastAPIï¼š`gunicorn -k uvicorn.workers.UvicornWorker -w 16`ã€‚
- Celeryï¼š`--concurrency=500 -P gevent`ã€‚
- ä»£ç†ï¼šç”¨çœŸå¯¦ APIï¼ˆå¦‚ 98IPï¼‰ï¼Œä¿æŒ 100-200 ä»£ç†ã€‚
- å£“åŠ›æ¸¬è©¦ï¼š
  ```bash
  locust -f locustfile.py
  ```

---

## ğŸ” åçˆ¬èŸ²ç­–ç•¥

- **ä»£ç†æ± **ï¼š`proxy_manager.py` å‹•æ…‹æ›´æ–° Redisã€‚
- **User-Agent**ï¼š`fake-useragent` éš¨æ©ŸåŒ–ã€‚
- **CAPTCHA**ï¼šæ”¯æ´ 2Captchaã€‚
- **é€²éš**ï¼šåŠ  Playwright è™•ç†å‹•æ…‹é é¢ã€‚

---

## ğŸ“‚ å°ˆæ¡ˆçµæ§‹

```plaintext
high_qps_crawler/
â”œâ”€â”€ laravel_api/               # Laravel API æ ¸å¿ƒ
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ Http/Controllers/CrawlerController.php
â”‚   â”‚   â”œâ”€â”€ Grpc/CrawlerServiceClient.php
â”‚   â”œâ”€â”€ proto/crawler.proto
â”‚   â”œâ”€â”€ routes/api.php
â”‚   â”œâ”€â”€ database/migrations/
â”‚   â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ fastapi_crawler/           # FastAPI çˆ¬èŸ²æ ¸å¿ƒ
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ crawler.py
â”‚   â”‚   â”œâ”€â”€ grpc_server.py
â”‚   â”‚   â”œâ”€â”€ proxy_manager.py
â”‚   â”œâ”€â”€ proto/crawler.proto
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ celery_workers/            # Celery å·¥ä½œç¯€é»
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ tasks.py
â”‚   â”‚   â”œâ”€â”€ celery_app.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ monitoring/                # ç›£æ§é…ç½®
â”‚   â”œâ”€â”€ prometheus.yml
â”‚   â””â”€â”€ logstash.conf
â”œâ”€â”€ redis_data/                # Redis æŒä¹…åŒ–
â”œâ”€â”€ data_storage/              # è³‡æ–™åº«æŒä¹…åŒ–
â”‚   â”œâ”€â”€ mongodb_data/
â”‚   â””â”€â”€ postgresql_data/
â””â”€â”€ docker-compose.yml         # Docker Compose é…ç½®
```

---

## ğŸ¤ è²¢ç»æ–¹å¼

1. Fork å°ˆæ¡ˆã€‚
2. é–‹åˆ†æ”¯ï¼š`git checkout -b feature/ä½ çš„åŠŸèƒ½`ã€‚
3. æäº¤ï¼š`git commit -m 'åŠ äº†å•¥å•¥'`ã€‚
4. æ¨é€ï¼š`git push origin feature/ä½ çš„åŠŸèƒ½`ã€‚
5. é–‹ Pull Requestã€‚
