# é«˜ QPS ç¶²è·¯çˆ¬èŸ²ç³»çµ±

é€™æ˜¯ä¸€å€‹é«˜æ•ˆèƒ½çš„åˆ†æ•£å¼ç¶²è·¯çˆ¬èŸ²ç³»çµ±ï¼Œç›®æ¨™æ˜¯é”åˆ° **æ¯ç§’ 10,000 æ¬¡æŸ¥è©¢ (QPS)**ï¼Œé©åˆå¤§è¦æ¨¡ç¶²é æŠ“å–ã€‚æœ¬å€‰åº«åªæ”¾æ ¸å¿ƒç¨‹å¼ç¢¼ï¼Œä½¿ç”¨ **Laravel**ã€**FastAPI**ã€**gRPC**ã€**Redis**ã€**Celery** å’Œ **PostgreSQL/MongoDB** æ‰“é€ ï¼Œæ¨¡çµ„åŒ–ä¸”å¯æ“´å±•ï¼Œæ”¯æ´åçˆ¬èŸ²å’Œç›£æ§åŠŸèƒ½ã€‚

---

## ğŸš€ å°ˆæ¡ˆç°¡ä»‹

é€™å€‹å°ˆæ¡ˆæ˜¯ç‚ºäº†è§£æ±ºé«˜ååé‡ç¶²é æŠ“å–çš„éœ€æ±‚ï¼Œæ ¸å¿ƒç¨‹å¼ç¢¼å±•ç¤ºå¦‚ä½•ç”¨åˆ†æ•£å¼æ¶æ§‹å¯¦ç¾é«˜æ•ˆçˆ¬èŸ²ã€‚ä¸»è¦ç‰¹è‰²ï¼š

- **é«˜æ€§èƒ½**ï¼šæ”¯æ´ 10,000 QPSï¼Œé  Docker Compose è¼•é¬†æ“´å±•ã€‚
- **æ¨¡çµ„åŒ–**ï¼šåˆ†æˆ Laravel APIã€FastAPI çˆ¬èŸ²ã€Redis ä½‡åˆ—å’Œ Celery è³‡æ–™è™•ç†ã€‚
- **åçˆ¬èŸ²**ï¼šæœ‰ä»£ç†æ± ã€éš¨æ©Ÿ User-Agent å’Œ 2Captcha æ”¯æ´ã€‚
- **ç›£æ§**ï¼šæ•´åˆ Prometheusã€Grafana å’Œ Celery Flowerï¼Œæ–¹ä¾¿è¿½è¹¤æ•ˆèƒ½ã€‚

**æ³¨æ„**ï¼šå€‰åº«åªåŒ…å«æ ¸å¿ƒç¨‹å¼ç¢¼ï¼ŒLaravel ç’°å¢ƒå¾—è‡ªå·±è£ã€‚

---

## ğŸ› ï¸ ç³»çµ±æ¶æ§‹

ç³»çµ±æ¶æ§‹ç°¡å–®æ˜ç­ï¼š

1. **Laravel API** (`laravel_api/`)ï¼š
   - è² è²¬ä»»å‹™æäº¤å’Œç‹€æ…‹æŸ¥è©¢ï¼Œé€é gRPC è·Ÿ FastAPI æºé€šã€‚
   - ä»»å‹™ç‹€æ…‹å­˜åœ¨ Redisï¼Œæ­·å²è³‡æ–™é€² PostgreSQL æˆ– MongoDBã€‚

2. **FastAPI çˆ¬èŸ²æ ¸å¿ƒ** (`fastapi_crawler/`)ï¼š
   - ç”¨ `aiohttp` å’Œ `httpx` æŠ“ç¶²é ï¼Œæ”¯æ´ä»£ç†æ± å’Œåçˆ¬èŸ²ç­–ç•¥ã€‚
   - æä¾› gRPC æœå‹™å’Œ REST ç«¯é»ã€‚

3. **Celery å·¥ä½œç¯€é»** (`celery_workers/`)ï¼š
   - è™•ç†æŠ“å›ä¾†çš„è³‡æ–™ï¼Œæ”¯æ´é«˜ä¸¦ç™¼å„²å­˜ã€‚

4. **Redis**ï¼šç®¡ä»»å‹™ä½‡åˆ—ã€ä»£ç†æ± å’Œ URL å»é‡ã€‚
5. **PostgreSQL/MongoDB**ï¼šé è¨­ PostgreSQLï¼Œå¯æ› MongoDBã€‚
6. **ç›£æ§**ï¼šPrometheusã€Grafana å’Œ Celery Flowerã€‚

---

## ğŸ“‹ ç’°å¢ƒéœ€æ±‚

- **Docker** å’Œ **Docker Compose**ï¼šè·‘å®¹å™¨åŒ–æœå‹™ã€‚
- **Laravel ç’°å¢ƒ**ï¼šPHP 8.2+ã€Composerï¼ˆè‡ªå·±è£ï¼‰ã€‚
- **ç¡¬é«”å»ºè­°**ï¼ˆ10,000 QPSï¼‰ï¼š
  - CPUï¼š32-64 æ ¸å¿ƒã€‚
  - è¨˜æ†¶é«”ï¼š128-256 GBã€‚
  - ç¶²è·¯ï¼š10 Gbpsã€‚
  - å„²å­˜ï¼šNVMe SSDï¼ˆ1-2 TBï¼‰ã€‚
- **ä»£ç† API**ï¼šåƒ 98IP æˆ– BrightData é€™ç¨®å¯é çš„ä»£ç†æœå‹™ã€‚
- **2Captcha API é‡‘é‘°**ï¼ˆå¯é¸ï¼‰ï¼šè§£ CAPTCHA ç”¨ã€‚

---

## ğŸ—ï¸ æ€éº¼è·‘èµ·ä¾†

1. **æŠ“ç¨‹å¼ç¢¼**ï¼š
   ```bash
   git clone https://github.com/BpsEason/high_qps_crawler.git
   cd high_qps_crawler
   ```

2. **è£ Laravel ç’°å¢ƒ**ï¼š
   - é€² `laravel_api/` ç›®éŒ„ï¼š
     ```bash
     cd laravel_api
     composer install
     cp .env.example .env
     php artisan key:generate
     ```

3. **è¨­ç’°å¢ƒè®Šæ•¸**ï¼š
   - åœ¨ `fastapi_crawler/.env`ï¼š
     ```env
     CAPTCHA_API_KEY=ä½ çš„_2captcha_api_é‡‘é‘°  # å¯é¸
     PROXY_API_URL=https://api.your-proxy-provider.com/get-proxies  # æ›æˆçœŸå¯¦ä»£ç† API
     PROXY_UPDATE_INTERVAL_SECONDS=3600
     ```
   - åœ¨ `laravel_api/.env`ï¼š
     ```env
     DB_CONNECTION=pgsql
     DB_HOST=pgbouncer
     DB_PORT=6432
     DB_DATABASE=crawler_db
     DB_USERNAME=user
     DB_PASSWORD=password
     REDIS_HOST=redis
     REDIS_PORT=6379
     FASTAPI_GRPC_HOST=fastapi_crawler:50051
     ```

4. **é¸è³‡æ–™åº«**ï¼š
   - é è¨­ç”¨ **PostgreSQL**ï¼ˆæœ‰ PgBouncer é€£ç·šæ± ï¼‰ã€‚
   - æƒ³ç”¨ **MongoDB**ï¼Œæ”¹ `docker-compose.yml` å’Œ `laravel_api/.env`ï¼š
     ```env
     DB_CONNECTION=mongodb
     DB_HOST=mongodb
     DB_PORT=27017
     DB_DATABASE=crawler_db
     MONGODB_URL=mongodb://mongodb:27017
     ```

5. **å»ºç½®ä¸¦å•Ÿå‹•æœå‹™**ï¼š
   ```bash
   docker compose build
   docker compose up -d
   ```

6. **åˆå§‹åŒ–è³‡æ–™åº«**ï¼š
   ```bash
   docker compose exec laravel php artisan migrate
   ```

7. **æ“´å±• Celery å·¥ä½œç¯€é»**ï¼š
   ```bash
   docker compose up -d --scale celery_worker=10
   ```

8. **è©¦è·‘çˆ¬èŸ²**ï¼š
   - ä¸Ÿå€‹ä»»å‹™ï¼š
     ```bash
     curl -X POST -H "Content-Type: application/json" -d '{"url": "http://example.com"}' http://localhost:8000/api/submit-crawl
     ```
   - æŸ¥ä»»å‹™ç‹€æ…‹ï¼š
     ```bash
     curl http://localhost:8000/api/crawl-status/{task_id}
     ```

---

## ğŸ”‘ æ ¸å¿ƒç¨‹å¼ç¢¼èˆ‡ä¸­æ–‡è¨»è§£

ä»¥ä¸‹æ˜¯å°ˆæ¡ˆçš„é—œéµç¨‹å¼ç¢¼ï¼Œå¸¶ä¸­æ–‡è¨»è§£ï¼Œå±•ç¤ºä¸»è¦åŠŸèƒ½ï¼š

### 1. Laravel API - ä»»å‹™æäº¤ (`laravel_api/app/Http/Controllers/CrawlerController.php`)

```php
<?php
namespace App\Http\Controllers;

use Illuminate\Http\Request;
use Grpc\ChannelCredentials;
use App\Grpc\CrawlerServiceClient;
use Illuminate\Support\Facades\Log;
use Illuminate\Support\Facades\Redis;

class CrawlerController extends Controller
{
    /**
     * æŠŠçˆ¬å–ä»»å‹™ä¸Ÿçµ¦ FastAPIï¼ˆç”¨ gRPCï¼‰
     */
    public function submitCrawlTask(Request $request)
    {
        // æª¢æŸ¥ URL æ ¼å¼
        $request->validate(['url' => 'required|url']);
        $url = $request->input('url');
        $taskId = uniqid('crawl_'); // ç”¢ç”Ÿå”¯ä¸€ä»»å‹™ ID

        try {
            // é€£ FastAPI çš„ gRPC æœå‹™
            $client = new CrawlerServiceClient(env('FASTAPI_GRPC_HOST', 'fastapi_crawler:50051'), [
                'credentials' => ChannelCredentials::createInsecure(),
            ]);
            
            // åŒ…è£ gRPC è«‹æ±‚
            $requestProto = new \Crawler\CrawlRequest();
            $requestProto->setUrl($url);
            $requestProto->setTaskId($taskId);

            // é€å‡ºè«‹æ±‚
            list($response, $status) = $client->Crawl($requestProto)->wait();
            if ($status->code !== \Grpc\STATUS_OK) {
                throw new \Exception("gRPC å¤±æ•—: " . $status->details);
            }

            // å­˜ä»»å‹™ç‹€æ…‹åˆ° Redis
            Redis::hset("crawl_task_status:{$taskId}", "status", "dispatched");
            Redis::hset("crawl_task_status:{$taskId}", "url", $url);
            Redis::hset("crawl_task_status:{$taskId}", "dispatched_at", now()->toIso8601String());

            Log::info("gRPC æˆåŠŸï¼ŒURL: $url, ä»»å‹™ ID: $taskId");
            return response()->json([
                'message' => 'ä»»å‹™é€å‡º',
                'task_id' => $taskId,
                'grpc_response_status' => $response->getStatus(),
            ]);
        } catch (\Exception $e) {
            // å‡ºéŒ¯å°±è¨˜ logï¼Œæ›´æ–° Redis ç‹€æ…‹
            Log::error("é€ä»»å‹™å¤±æ•—ï¼ŒURL: $url. éŒ¯èª¤: " . $e->getMessage());
            Redis::hset("crawl_task_status:{$taskId}", "status", "submission_failed");
            Redis::hset("crawl_task_status:{$taskId}", "error", $e->getMessage());
            return response()->json(['error' => 'é€ä»»å‹™å¤±æ•—: ' . $e->getMessage()], 500);
        }
    }
}
```

**èªªæ˜**ï¼šé€™æ®µç¨‹å¼ç¢¼è² è²¬æŠŠçˆ¬å–ä»»å‹™ä¸Ÿçµ¦ FastAPIï¼Œç”¨ gRPC é€šä¿¡ï¼Œä»»å‹™ç‹€æ…‹å­˜åˆ° Redisï¼Œæ–¹ä¾¿å³æ™‚æŸ¥è©¢ã€‚

---

### 2. FastAPI çˆ¬èŸ²æ ¸å¿ƒ - æŠ“ç¶²é  (`fastapi_crawler/app/crawler.py`)

```python
import aiohttp
import httpx
from fake_useragent import UserAgent
import asyncio
import redis.asyncio as aioredis
import random
import os

REDIS_HOST = os.getenv("REDIS_HOST", "redis")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))

async def get_redis_client():
    """é€£ Redisï¼Œæ‹¿ä»£ç†æˆ–å»é‡"""
    global _redis_client
    if _redis_client is None:
        _redis_client = await aioredis.from_url(f"redis://{REDIS_HOST}:{REDIS_PORT}")
    return _redis_client

async def get_proxy():
    """å¾ Redis æŠ“å€‹å¥½ç”¨çš„ä»£ç†"""
    client = await get_redis_client()
    try:
        proxies_bytes = await client.smembers("proxy_pool")
        if not proxies_bytes:
            print("æ²’ä»£ç†ï¼Œç›´æ¥é€£")
            return None
        proxies = [p.decode('utf-8') for p in proxies_bytes]
        random.shuffle(proxies)
        for proxy in proxies:
            try:
                async with httpx.AsyncClient(proxies={'http://': proxy, 'https://': proxy}, timeout=5) as test_client:
                    response = await test_client.get("http://httpbin.org/ip")
                    response.raise_for_status()
                print(f"ç”¨é€™å€‹ä»£ç†: {proxy}")
                return proxy
            except (httpx.RequestError, httpx.HTTPStatusError) as e:
                print(f"ä»£ç† {proxy} å£äº†: {e}")
                await client.srem("proxy_pool", proxy)
        print("æ²’å¥½ä»£ç†")
        return None
    except Exception as e:
        print(f"æŠ“ä»£ç†å¤±æ•—: {e}")
        return None

async def perform_crawl(url: str, task_id: str) -> str:
    """æŠ“ç¶²é å…§å®¹"""
    user_agent = UserAgent().random  # éš¨æ©Ÿæ› User-Agent
    proxy = await get_proxy()  # æ‹¿ä»£ç†
    await asyncio.sleep(random.uniform(0.1, 1.0))  # éš¨æ©Ÿç­‰ä¸€ä¸‹ï¼Œèº²åçˆ¬

    headers = {
        'User-Agent': user_agent,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    }
    
    async with httpx.AsyncClient(headers=headers, proxies={'http://': proxy, 'https://': proxy} if proxy else None, 
                                 timeout=30, follow_redirects=True) as client:
        try:
            print(f"æŠ“: {url} (ä»»å‹™ ID: {task_id})")
            response = await client.get(url)
            response.raise_for_status()
            redis_client = await get_redis_client()
            await redis_client.sadd("crawled_urls", url)  # è¨˜ä½æŠ“éçš„ URL
            return response.text
        except httpx.HTTPStatusError as e:
            print(f"æŠ“å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ {e.response.status_code}: {e}")
            if e.response.status_code in [403, 429]:
                print("å¯èƒ½æœ‰ CAPTCHA")
            raise
```

**èªªæ˜**ï¼šé€™æ®µç¨‹å¼ç¢¼ç”¨éåŒæ­¥æ–¹å¼æŠ“ç¶²é ï¼Œæ”¯æ´ä»£ç†å’Œéš¨æ©Ÿ User-Agentï¼Œé‚„æœ‰ URL å»é‡ï¼Œé ç•™äº† CAPTCHA è™•ç†ç©ºé–“ã€‚

---

### 3. Celery å·¥ä½œç¯€é» - å­˜è³‡æ–™ (`celery_workers/app/tasks.py`)

```python
from celery_app import app
import asyncio
import asyncpg
import datetime
import os
import redis.asyncio as aioredis

POSTGRES_URL = os.getenv("POSTGRES_URL", "postgresql://user:password@pgbouncer:6432/crawler_db")
REDIS_HOST = os.getenv("REDIS_HOST", "redis")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))

async def get_redis_client():
    """é€£ Redisï¼Œæ›´æ–°ä»»å‹™ç‹€æ…‹"""
    return await aioredis.from_url(f"redis://{REDIS_HOST}:{REDIS_PORT}")

async def get_pg_pool():
    """æ‹¿ PostgreSQL é€£ç·šæ± """
    global _pg_pool
    if _pg_pool is None:
        if POSTGRES_URL:
            _pg_pool = await asyncpg.create_pool(POSTGRES_URL, min_size=10, max_size=50)
        else:
            raise ValueError("æ²’è¨­ POSTGRES_URL")
    return _pg_pool

@app.task(name='tasks.store_data', bind=True, default_retry_delay=300, max_retries=5)
def store_data_task(self, url: str, content: str, task_id: str = None):
    """æŠŠæŠ“åˆ°çš„è³‡æ–™å­˜èµ·ä¾†"""
    print(f"è™•ç† URL: {url} (ä»»å‹™ ID: {task_id})")
    redis_client_sync = asyncio.run(get_redis_client())
    try:
        data_to_store = [{
            "url": url,
            "content": content,
            "crawled_at": datetime.datetime.now(),
            "task_id": task_id,
            "title": "æ¨™é¡Œä½”ä½",
            "metadata": {}
        }]

        asyncio.run(_store_data_async(data_to_store))
        
        asyncio.run(redis_client_sync.hset(f"crawl_task_status:{task_id}", mapping={
            "status": "completed",
            "completed_at": datetime.datetime.now().isoformat()
        }))
        print(f"å­˜å¥½: {url}")
        return f"å­˜å¥½: {url}"
    except Exception as e:
        print(f"å­˜å¤±æ•—: {url}, éŒ¯èª¤: {e}")
        asyncio.run(redis_client_sync.hset(f"crawl_task_status:{task_id}", mapping={
            "status": "failed",
            "error": str(e),
            "completed_at": datetime.datetime.now().isoformat()
        }))
        raise self.retry(exc=e)
    finally:
        asyncio.run(redis_client_sync.close())

async def _store_data_async(data_items: list):
    """éåŒæ­¥å­˜åˆ° PostgreSQL"""
    if POSTGRES_URL:
        pool = await get_pg_pool()
        async with pool.acquire() as conn:
            await conn.executemany('''
                INSERT INTO crawled_data(url, content, crawled_at, title, metadata, task_id)
                VALUES($1, $2, $3, $4, $5::jsonb, $6)
                ON CONFLICT (url) DO UPDATE
                SET content = EXCLUDED.content, crawled_at = EXCLUDED.crawled_at,
                    title = EXCLUDED.title, metadata = EXCLUDED.metadata,
                    updated_at = NOW();
            ''', [(item['url'], item['content'], item['crawled_at'], item.get('title', ''),
                   item.get('metadata', {}), item['task_id']) for item in data_items])
        print(f"å­˜äº† {len(data_items)} æ¢è³‡æ–™")
```

**èªªæ˜**ï¼šé€™æ®µç¨‹å¼ç¢¼æŠŠæŠ“åˆ°çš„è³‡æ–™å­˜åˆ° PostgreSQLï¼Œç”¨é€£ç·šæ± å’Œæ‰¹é‡æ’å…¥æé«˜æ•ˆç‡ï¼ŒåŒæ™‚æ›´æ–° Redis çš„ä»»å‹™ç‹€æ…‹ã€‚

---

## â“ å¸¸è¦‹å•é¡Œèˆ‡è§£ç­”

ä»¥ä¸‹æ˜¯ä¸€äº›é–‹ç™¼è€…å¯èƒ½æœƒå•çš„å•é¡Œï¼Œçµåˆå°ˆæ¡ˆç¨‹å¼ç¢¼å’Œè¨­è¨ˆï¼Œå¹«ä½ å¿«é€Ÿä¸Šæ‰‹æˆ–è§£æ±ºç–‘æƒ‘ã€‚

### Q1: æ€éº¼è®“é€™ç³»çµ±è·‘å‡ºé«˜ QPSï¼Ÿ

é€™å°ˆæ¡ˆç”¨åˆ†æ•£å¼æ¶æ§‹ï¼Œé å¤šå€‹çµ„ä»¶åˆ†å·¥ä¾†æ’é«˜ QPSã€‚Laravel ç®¡ä»»å‹™åˆ†æ´¾ï¼ŒFastAPI è² è²¬æŠ“ç¶²é ï¼ŒCelery è™•ç†è³‡æ–™å„²å­˜ï¼ŒRedis ç•¶ä¸­é–“äººã€‚é—œéµæ˜¯ï¼š

- **éåŒæ­¥çˆ¬å–**ï¼šFastAPI ç”¨ `aiohttp` å’Œ `httpx` è·‘éåŒæ­¥è«‹æ±‚ï¼Œå–®ç¯€é»èƒ½åˆ° 1,600-4,000 QPSã€‚
- **åˆ†æ•£å¼è™•ç†**ï¼šCelery ç”¨ `gevent` æ¨¡å¼ï¼Œæ¯å€‹ worker è·‘ 300 å€‹ä»»å‹™ã€‚
- **æ“´å±•**ï¼šç”¨ `docker compose up -d --scale celery_worker=10` é–‹å¤šå€‹ workerã€‚

**ç¨‹å¼ç¢¼**ï¼ˆ`fastapi_crawler/app/crawler.py`ï¼‰ï¼š
```python
async def perform_crawl(url: str, task_id: str) -> str:
    user_agent = UserAgent().random
    proxy = await get_proxy()
    await asyncio.sleep(random.uniform(0.1, 1.0))  # éš¨æ©Ÿå»¶é²
    headers = {'User-Agent': user_agent, ...}
    async with httpx.AsyncClient(headers=headers, proxies={'http://': proxy} if proxy else None,
                                 timeout=30, follow_redirects=True) as client:
        response = await client.get(url)
        response.raise_for_status()
        redis_client = await get_redis_client()
        await redis_client.sadd("crawled_urls", url)
        return response.text
```

**å°è¨£ç«…**ï¼š
- è¨­çœŸå¯¦çš„ `PROXY_API_URL`ï¼ˆåƒ 98IPï¼‰ï¼Œä¿æŒ 100-200 å€‹ä»£ç†ã€‚
- é–‹å¤šé» FastAPI workerï¼š`gunicorn -w 16`ã€‚
- ç”¨ Locust æ¸¬ QPSï¼š
  ```bash
  locust -f locustfile.py
  ```

---

### Q2: è³‡æ–™åº«æ€éº¼æ‰›é«˜ä¸¦ç™¼ï¼Ÿ

å°ˆæ¡ˆç”¨ **PostgreSQL** é… **PgBouncer** é€£ç·šæ± ï¼ŒCelery é€é `asyncpg` æ‰¹é‡å¯«è³‡æ–™ï¼Œæ¸›å°‘é€£ç·šè² æ“”ã€‚å¦‚æœé¸ MongoDBï¼Œå‰‡é å®ƒçš„éˆæ´»çµæ§‹è™•ç†éçµæ§‹åŒ–è³‡æ–™ã€‚

**ç¨‹å¼ç¢¼**ï¼ˆ`celery_workers/app/tasks.py`ï¼‰ï¼š
```python
async def _store_data_async(data_items: list):
    pool = await get_pg_pool()  # é€£ç·šæ± ï¼Œ10-50 é€£ç·š
    async with pool.acquire() as conn:
        await conn.executemany('''
            INSERT INTO crawled_data(url, content, crawled_at, title, metadata, task_id)
            VALUES($1, $2, $3, $4, $5::jsonb, $6)
            ON CONFLICT (url) DO UPDATE
            SET content = EXCLUDED.content, ...
        ''', [(item['url'], item['content'], item['crawled_at'], ...) for item in data_items])
    print(f"å­˜äº† {len(data_items)} æ¢è³‡æ–™")
```

**åšæ³•**ï¼š
- PgBouncer è¨­ `MAX_CLIENT_CONNECTIONS=1000`ï¼Œ`DEFAULT_POOL_SIZE=50`ã€‚
- ç”¨ `ON CONFLICT` å»é‡ï¼ŒçœæŸ¥è©¢æ™‚é–“ã€‚
- åœ¨ `url` æ¬„ä½åŠ å”¯ä¸€ç´¢å¼•ã€‚
- MongoDB å‰‡å¯è€ƒæ…®åˆ†ç‰‡ï¼Œè™•ç†è¶…å¤§è³‡æ–™é‡ã€‚

---

### Q3: æ€éº¼èº²ç¶²ç«™çš„åçˆ¬èŸ²ï¼Ÿ

å°ˆæ¡ˆæœ‰å¹¾æ‹›é˜²åçˆ¬ï¼š

1. **ä»£ç†æ± **ï¼š`proxy_manager.py` å®šæœŸå¾ API æŠ“ä»£ç†ï¼Œå­˜ Redisï¼Œè‡ªå‹•æª¢æŸ¥å¥åº·ã€‚
2. **User-Agent**ï¼šç”¨ `fake-useragent` éš¨æ©Ÿæ›ã€‚
3. **CAPTCHA**ï¼šæ”¯æ´ 2Captchaï¼Œé ç•™è§£æé‚è¼¯ã€‚

**ç¨‹å¼ç¢¼**ï¼ˆ`fastapi_crawler/app/crawler.py`ï¼‰ï¼š
```python
async def get_proxy():
    client = await get_redis_client()
    proxies_bytes = await client.smembers("proxy_pool")
    proxies = [p.decode('utf-8') for p in proxies_bytes]
    random.shuffle(proxies)
    for proxy in proxies:
        try:
            async with httpx.AsyncClient(proxies={'http://': proxy, 'https://': proxy}, timeout=5) as test_client:
                response = await test_client.get("http://httpbin.org/ip")
                response.raise_for_status()
            return proxy
        except:
            await client.srem("proxy_pool", proxy)
    return None
```

**å»ºè­°**ï¼š
- è¨­éš¨æ©Ÿå»¶é²ï¼ˆ0.1-1.0 ç§’ï¼‰ã€‚
- æœªä¾†åŠ  Playwright æŠ“å‹•æ…‹é é¢ï¼š
  ```yaml
  playwright:
    image: mcr.microsoft.com/playwright:v1.39.0
  ```

---

### Q4: æ€éº¼çœ‹ç³»çµ±è·‘å¾—é †ä¸é †ï¼Ÿ

å°ˆæ¡ˆæ•´åˆäº†å¹¾å€‹ç›£æ§å·¥å…·ï¼š

- **Prometheus**ï¼š`http://localhost:9090`ï¼Œçœ‹ QPSã€å»¶é²ã€ä»£ç†æ•¸ã€‚
- **Grafana**ï¼š`http://localhost:3000`ï¼ˆ`admin/admin`ï¼‰ï¼Œç•«åœ–è¡¨ã€‚
- **Celery Flower**ï¼š`http://localhost:5555`ï¼ŒæŸ¥ä»»å‹™ç‹€æ…‹ã€‚

**èª¿å„ª**ï¼š
- åŠ  FastAPI workerï¼š`gunicorn -w 16`ã€‚
- èª¿ Celery ä¸¦ç™¼ï¼š`--concurrency=500 -P gevent`ã€‚
- ç”¨ Locust å£“åŠ›æ¸¬è©¦ï¼Œæ‰¾ç“¶é ¸ã€‚

---

### Q5: ç‚ºå•¥ç”¨ gRPC é€£ Laravel è·Ÿ FastAPIï¼Ÿ

gRPC æ¯” REST å¿«ï¼Œé©åˆå…§éƒ¨é«˜é »é€šä¿¡ï¼š

- **é«˜æ•ˆ**ï¼šç”¨ HTTP/2 å’Œ Protocol Buffersï¼Œåºåˆ—åŒ–å¿«ã€‚
- **å‹åˆ¥å®‰å…¨**ï¼š`crawler.proto` å®šç¾©è³‡æ–™çµæ§‹ï¼Œå°‘å‡ºéŒ¯ã€‚
- **æ“´å±•æ€§**ï¼šæ”¯æ´é›™å‘æµï¼Œæœªä¾†å¯åŠ å³æ™‚å›é¥‹ã€‚

**ç¨‹å¼ç¢¼**ï¼ˆ`laravel_api/app/Grpc/CrawlerServiceClient.php`ï¼‰ï¼š
```php
$client = new CrawlerServiceClient(env('FASTAPI_GRPC_HOST', 'fastapi_crawler:50051'), [
    'credentials' => ChannelCredentials::createInsecure(),
]);
$requestProto = new \Crawler\CrawlRequest();
$requestProto->setUrl($url);
$requestProto->setTaskId($taskId);
list($response, $status) = $client->Crawl($requestProto)->wait();
```

**å•¥æ™‚ç”¨ REST**ï¼Ÿå¦‚æœè·¨èªè¨€æˆ–å¿«é€Ÿè©¦é©—ï¼ŒREST ç°¡å–®é»ï¼Œä½† gRPC æ›´é©åˆé€™å°ˆæ¡ˆçš„é«˜æ€§èƒ½éœ€æ±‚ã€‚

---

### Q6: ä»»å‹™æ›äº†æ€éº¼è¾¦ï¼Ÿ

å°ˆæ¡ˆç”¨ Celery çš„é‡è©¦æ©Ÿåˆ¶å’Œ Redis ç‹€æ…‹è¿½è¹¤è™•ç†å¤±æ•—ï¼š

- **é‡è©¦**ï¼š`store_data_task` è¨­ `max_retries=5`ï¼Œå¤±æ•—å¾Œç­‰ 300 ç§’å†è©¦ã€‚
- **ç‹€æ…‹**ï¼šå¤±æ•—æ™‚æ›´æ–° Redis çš„ `crawl_task_status:{taskId}`ã€‚

**ç¨‹å¼ç¢¼**ï¼ˆ`celery_workers/app/tasks.py`ï¼‰ï¼š
```python
@app.task(name='tasks.store_data', bind=True, default_retry_delay=300, max_retries=5)
def store_data_task(self, url: str, content: str, task_id: str = None):
    try:
        data_to_store = [{"url": url, "content": content, ...}]
        asyncio.run(_store_data_async(data_to_store))
        asyncio.run(redis_client_sync.hset(f"crawl_task_status:{task_id}", mapping={
            "status": "completed",
            "completed_at": datetime.datetime.now().isoformat()
        }))
    except Exception as e:
        asyncio.run(redis_client_sync.hset(f"crawl_task_status:{task_id}", mapping={
            "status": "failed",
            "error": str(e),
            ...
        }))
        raise self.retry(exc=e)
```

**å°æŠ€å·§**ï¼š
- è¨­åˆç†çš„ `default_retry_delay`ï¼Œåˆ¥è®“é‡è©¦æŠŠç³»çµ±å£“å®ã€‚
- éŒ¯èª¤è¨˜åˆ° Redisï¼Œæ–¹ä¾¿ debugã€‚

---

## ğŸ“Š ç›£æ§èˆ‡é™¤éŒ¯

- **Prometheus**ï¼š`http://localhost:9090`ã€‚
- **Grafana**ï¼š`http://localhost:3000`ï¼ˆ`admin/admin`ï¼‰ã€‚
- **Celery Flower**ï¼š`http://localhost:5555`ã€‚
- **ELK Stack**ï¼ˆå¯é¸ï¼‰ï¼šæ”¹ `docker-compose.yml`ï¼Œç”¨ `http://localhost:5601`ã€‚

---

## âš™ï¸ æ€éº¼è·‘æ›´å¿«

æƒ³è¡ **10,000 QPS**ï¼š
- FastAPIï¼š`gunicorn -w 16`ã€‚
- Celeryï¼š`--concurrency=500 -P gevent`ã€‚
- ä»£ç†ï¼šè¨­çœŸå¯¦ `PROXY_API_URL`ï¼Œä¿æŒ 100-200 å€‹ä»£ç†ã€‚
- æ¸¬æ•ˆèƒ½ï¼š
  ```bash
  locust -f locustfile.py
  ```

---

## ğŸ” åçˆ¬èŸ²æ‹›æ•¸

- **ä»£ç†**ï¼š`proxy_manager.py` ç®¡ Redis ä»£ç†æ± ã€‚
- **User-Agent**ï¼š`fake-useragent` éš¨æ©ŸåŒ–ã€‚
- **CAPTCHA**ï¼šæ”¯æ´ 2Captchaã€‚
- **é€²éš**ï¼šåŠ  Playwrightï¼š
  ```yaml
  playwright:
    image: mcr.microsoft.com/playwright:v1.39.0
  ```

---

## ğŸ“‚ å°ˆæ¡ˆçµæ§‹

```plaintext
high_qps_crawler/
â”œâ”€â”€ laravel_api/               # Laravel API
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ Http/Controllers/CrawlerController.php
â”‚   â”‚   â”œâ”€â”€ Grpc/CrawlerServiceClient.php
â”‚   â”œâ”€â”€ proto/crawler.proto
â”‚   â”œâ”€â”€ routes/api.php
â”‚   â”œâ”€â”€ database/migrations/
â”‚   â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ fastapi_crawler/           # FastAPI çˆ¬èŸ²
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ crawler.py
â”‚   â”‚   â”œâ”€â”€ grpc_server.py
â”‚   â”‚   â”œâ”€â”€ proxy_manager.py
â”‚   â”œâ”€â”€ proto/crawler.proto
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ celery_workers/            # Celery å·¥ä½œç¯€é»
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ tasks.py
â”‚   â”‚   â”œâ”€â”€ celery_app.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ monitoring/                # ç›£æ§
â”‚   â”œâ”€â”€ prometheus.yml
â”‚   â””â”€â”€ logstash.conf
â”œâ”€â”€ redis_data/                # Redis è³‡æ–™
â”œâ”€â”€ data_storage/              # è³‡æ–™åº«
â”‚   â”œâ”€â”€ mongodb_data/
â”‚   â””â”€â”€ postgresql_data/
â””â”€â”€ docker-compose.yml         # Docker Compose
```

---

## ğŸ¤ æƒ³å¹«å¿™ï¼Ÿ

1. Fork é€™å°ˆæ¡ˆã€‚
2. é–‹åˆ†æ”¯ï¼š`git checkout -b feature/ä½ çš„åŠŸèƒ½`ã€‚
3. æäº¤ï¼š`git commit -m 'åŠ äº†å•¥å•¥'`ã€‚
4. æ¨ä¸Šå»ï¼š`git push origin feature/ä½ çš„åŠŸèƒ½`ã€‚
5. é–‹ Pull Requestã€‚

